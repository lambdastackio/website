<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ceph on The LambdaStack Project</title>
    <link>http://lambdastack.io/categories/ceph/index.xml</link>
    <description>Recent content in Ceph on The LambdaStack Project</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;p&gt;Copyright (c) 2017, Chris Jones; all rights reserved. All comments and views are my own and not my employer&#39;s.&lt;/p&gt;&lt;p&gt;All content originating from Chris Jones is licensed under &lt;a href=&#34;https://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;Attribution-ShareAlike 4.0 International&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/cjones303/&#34;&gt;Click here to contact me.&lt;/a&gt;&lt;/p&gt;</copyright>
    <atom:link href="http://lambdastack.io/categories/ceph/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>AWS S3 vs On-Premises</title>
      <link>http://lambdastack.io/blog/2017/03/09/aws-vs-onprem/</link>
      <pubDate>Thu, 09 Mar 2017 20:00:00 +0200</pubDate>
      
      <guid>http://lambdastack.io/blog/2017/03/09/aws-vs-onprem/</guid>
      <description>

&lt;h2 id=&#34;aws-s3-vs-on-premises&#34;&gt;AWS S3 vs On-Premises&lt;/h2&gt;

&lt;p&gt;When you own Enterprise Storage or are asked to build enterprise class storage you find yourself more frequently having to cost justify against outside Cloud Storage vendors like AWS S3, GCE or Azure. So, how will you do that since you have to make capital expenditures vs OpEx that these providers discuss so much?&lt;/p&gt;

&lt;p&gt;When you break down the pricing of AWS S3 and others you find that the actual storage cost is not too bad (depending on amount of stored data). What you don&amp;rsquo;t realize is the network transfer rate is where they get you. I have heard stories of small firms having bet pools going just to see who is closer at guessing a given days AWS cost. The same can be said for enterprises but the stakes are much higher!&lt;/p&gt;

&lt;h4 id=&#34;sidebar&#34;&gt;Sidebar&lt;/h4&gt;

&lt;p&gt;I like AWS, Azure and Google. They serve a good purpose of helping you get started on projects at a low entry cost and the &amp;ldquo;pay-as-you-go&amp;rdquo; model is very compelling. This model changes over time for some enterprises especially when it comes to storage. I just recently had a Senior level &amp;ldquo;Amazonian&amp;rdquo; (that&amp;rsquo;s what they call themselves) say &amp;ldquo;AWS is the world largest startup and that&amp;rsquo;s how we operate.&amp;rdquo; That sounds kinda cool but when I pressed him on enterprise like questions, his story fell flat. AWS seems to believe everything should run in a public cloud but that&amp;rsquo;s just not wise (in a general sense - some enterprises can&amp;rsquo;t allow their data outside their datacenters for many reasons).  Azure actually understands this and has &amp;ldquo;on-premises&amp;rdquo; versions of some of their offerings. They seem to have more of an understanding of the enterprise than the others.&lt;/p&gt;

&lt;p&gt;Again, I like AWS. I wrote the Rust AWS-SDK-RUST SDK and other S3 tools. I&amp;rsquo;m simply making honest comparisons of cloud providers for enterprises. I have also used Google on past contracts and they are very good but I don&amp;rsquo;t have an enterprise reference to compare against.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Also, don&amp;rsquo;t get confused by AWS&amp;rsquo; &amp;ldquo;manage your on-premises via AWS.&amp;rdquo; It&amp;rsquo;s only their console that you can plug some data into and not a true on-premises solution.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;how-to-cost-justify&#34;&gt;How to cost justify&lt;/h4&gt;

&lt;p&gt;Back from my sidebar. How do we cost justify on-premises S3 and compare it to AWS S3? The first thing is &lt;strong&gt;DO NOT&lt;/strong&gt; use a tradition enterprise storage vendor - you gain nothing. You need a solid open source solution using COTS (Common Off The Shelf) hardware such as Ceph on any hardware platform that meets your budget and minimum requirements to run a good S3 Object Store.&lt;/p&gt;

&lt;p&gt;Cost Example (deep storage): (NB: These are conservative costs. You will need to plug in your costs. This also assumes you use all of the storage and most of data is machine generated as in Big Data. Also, no operating labor cost is factored in since that is so variable [see summary].)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Single datacenter. Data is very durable with Erasure Coding.&lt;/p&gt;

&lt;p&gt;5 Racks ($300K each) - $1,500,000 (assumes your own datacenters with power/cooling - cost factors in ~10 spare storage nodes and additional 10TB HDD for replacements). Also these figures are high so your real figures should be a good bit less depending on your hardware vendor and even lower if you don&amp;rsquo;t have to use outside vendors and contractors.&lt;/p&gt;

&lt;p&gt;Includes racks, high-density storage, network spines, 20Gbs NICs, TORs, software load-balancers (HAProxy), install labor, spares, etc&lt;/p&gt;

&lt;p&gt;Ceph S3, Erasure Coding &lt;sup&gt;8&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; (11 shards - &lt;a href=&#34;http://lambdastack.io/blog/2017/02/26/erasure-coding/&#34;&gt;see other article on EC calculations&lt;/a&gt;) - tuned for larger objects&lt;/p&gt;

&lt;p&gt;10TB HDD x 204 (drives per rack) x 5 (racks) = 10.2PB raw storage / 1.375 (EC factor - see EC article) = 7,418TB (7.4PB) usable. Using EC instead of replica for better storage utilization. ~3% write overhead and ~1.2% read overhead for EC as compared to replicas. The big penalty will come on a rebuild of an object if required due to data failure etc.&lt;/p&gt;

&lt;p&gt;$1,500,000 (total cost) / 7,418TB (usable) = $202.21 per TB (internally price this one-time, annually or however you like. This example will be one-time (1 year) divided into monthly)&lt;/p&gt;

&lt;p&gt;Operational costs (power/cooling/space density) are standardize (PUE). A normal rack uses less than ~5kW on average and I will use $.10 kW/h cost (change to your areas average): 8,760 (annual hours) * $.10 * 5 = $4,104.06 * 5 (racks) = $20,520.30 (annual) / 7,418TB (usable) = $2.77TB for OpEx (annual)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;$202.21 + $2.77 = $204.98TB (first year). $2.77TB annual after first year plus operational labor&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;NB: If you use commercial hardware load-balancers then your cost will increase. These do not usually gain you performance over HAProxy in this scenario. However, you can always add them and repurpose the HAProxy nodes included above.&lt;/p&gt;

&lt;p&gt;Network traffic - ~15Gbs sustained (give or take) split with &lt;sup&gt;70&lt;/sup&gt;&amp;frasl;&lt;sub&gt;30&lt;/sub&gt; read/write (change based on your assumptions but these values are used for standard benchmarking). Since a high-speed network is part of a normal enterprise (large), there are no additional costs for network usage in this scenario beyond purchasing additional spines which are factored into the cost. (Internal only)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;AWS S3 (published pricing):&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Store 7,418TB to be equal with above&lt;/p&gt;

&lt;p&gt;Replication with datacenter (standard). Replication to other regions will cost more.&lt;/p&gt;

&lt;p&gt;Transfer In &lt;a href=&#34;https://aws.amazon.com/snowmobile/&#34;&gt;(Would take a &amp;ldquo;snowmobile&amp;rdquo; - Big semi-tractor and trailer rigs)&lt;/a&gt;. This scenario doesn&amp;rsquo;t even cover that cost but it&amp;rsquo;s expensive and takes a long time.&lt;/p&gt;

&lt;p&gt;Transfer Out - 70TB monthly (for this simple low cost example) - This applies to everything including EC2, Direct Connect etc.&lt;/p&gt;

&lt;p&gt;Direct Connect - Would need 2 LAG 10Gbs circuits plus additional equipment to sustain internal rate above. The cost is not reported here and much higher (setup, monthly and long-term contract)!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;$176,110.98 (monthly) * 12 = $2,113,331.76 (annual) / 7,418TB = $284.89TB (annual) PLUS costs not reported!!&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;http://lambdastack.io/img/aws-s3-worksheet.png&#34; style=&#34;max-width: 100%&#34;&gt;
&lt;strong&gt;AWS online worksheet - &lt;a href=&#34;https://calculator.s3.amazonaws.com/index.html&#34;&gt;https://calculator.s3.amazonaws.com/index.html&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://lambdastack.io/img/aws-s3-detail.png&#34; style=&#34;max-width: 100%&#34;&gt;
&lt;strong&gt;AWS detail&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&#34;summary&#34;&gt;Summary&lt;/h4&gt;

&lt;p&gt;If you are a startup and only using a small amount of storage then you can easily cost justify using a public cloud provider. However, if you are an enterprise and own datacenters then it&amp;rsquo;s far better from a cost stand point to build your own S3 on-premises and maintain control (even with added operational labor costs). The above scenario takes into consideration only one datacenter for the on-premises so the cost look very good for it even starting in the first year. However, if you factor in a second datacenter, just double the cost to keep it easy, then you see that year one (only) is better (from a cash outlay) for AWS S3 (w/o high transfer rate and Direct Connect costs).&lt;/p&gt;

&lt;p&gt;Second datacenter factored in for on-premises:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;$3,000,000 for 2 datacenters with ~10 spare servers and drives per datacenter.&lt;/p&gt;

&lt;p&gt;$204.98TB x 2 = $409.96TB (annual) or $409.96TB / 3 (3 years) = $136.65TB annually over 3 year period.&lt;/p&gt;

&lt;p&gt;Factor in replication costs of AWS S3 to other regions if comparing with multiple datacenters of on-premises.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Using the above cost figures, the AWS S3 cost will remain annual while the on-premises&amp;rsquo; cost go down annually due to many factors but mainly due to shelf life of 3-5yrs of equipment. Of course, there will be the annual OpEx costs mentioned plus drive and server replacement costs (some were already factored into the initial cost). The on-premises version also does not include operational labor costs (important). However, the labor cost would most likely only get you closer on year one using the single datacenter scenario.&lt;/p&gt;

&lt;p&gt;The point is, there are many ways to approach the cost points. If you have a lot of data and it continues to grow at an exponential rate and you continue to find more ways to process (analyze) the data then your transfer out of storage rates will increase (same transfer costs apply to EC2 and Direct Connect) and thus your real cost of AWS S3. This can also be applied to AWS Glacier as well.&lt;/p&gt;

&lt;p&gt;Again, AWS S3 is great but it is more expensive than you think when you have a lot of data and need access to it frequently. But if you&amp;rsquo;re a small shop or just want someone else to manage things for you, it&amp;rsquo;s a good option. Just don&amp;rsquo;t assume it&amp;rsquo;s cheaper because it&amp;rsquo;s not especially when you get into PB range which is now becoming a norm.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;Tested S3 from EC2 large in us-east-1 (same) region using JMeter and used same EC2 JMeter tests against Ceph in DMZ (owned datacenter - not close to region) and Ceph won in performance in most tests. Was not testing for performances between the two but was testing to establish a base line. Results were very surprising! Take away - do a lot of tests because results can be counter intuitive.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Erasure Coding vs Replica</title>
      <link>http://lambdastack.io/blog/2017/02/26/erasure-coding/</link>
      <pubDate>Sun, 26 Feb 2017 11:27:27 -0400</pubDate>
      
      <guid>http://lambdastack.io/blog/2017/02/26/erasure-coding/</guid>
      <description>

&lt;h4 id=&#34;erasure-coding-vs-replica&#34;&gt;Erasure Coding vs Replica&lt;/h4&gt;

&lt;p&gt;Ceph RadosGW (RGW), Ceph&amp;rsquo;s S3 Object Store, supports both Replica and Erasure Coding. Most all examples of using RGW show replicas because that&amp;rsquo;s the easiest to setup, manage and get your head around. Replicas simply means that a default of 3 means that RGW stores the original plus two more copies spread out within the cluster based on the Crush Map, Ceph&amp;rsquo;s way of calculating where to store objects.&lt;/p&gt;

&lt;p&gt;Erasure coding is a form of durability calculations that allows you to maintain the same or better durability as replicas but at a much better density model. Meaning, if I have three full racks of storage using the default replicas and I have set the crush map up to keep each replica on a different rack then I really only have one full rack of actual storage since the other two are used for durability. With an erasure code of &lt;code&gt;8/3&lt;/code&gt; I will have approximately 1.5 racks of actual storage which is much better. However, erasure coding comes with a cost.&lt;/p&gt;

&lt;p&gt;In the scenario of &lt;code&gt;8/3&lt;/code&gt; previously mentioned, it means that I have 8 data chunks plus 3 parity chunks for a total of 11 chunks. These chunks are then spread out over the cluster using a different crush map rule that basically says &amp;lsquo;put each object on a different node and spread them between different racks and nodes as evenly as possible&amp;rsquo;. This means that you can lose 3 objects before you would lose any data. Which also means it takes at least 8 objects to form the complete object. Different size clusters and different durability factors will dictate what &lt;code&gt;k/m (data/parity)&lt;/code&gt; value your decide on for you cluster.&lt;/p&gt;

&lt;p&gt;Earlier, I mentioned that erasure coding comes at a cost. The cost is taxed on each GET (read) and PUT (write). It takes time to calculate the parity on the PUT and time to assemble the objects on the GET. Since RGW is a guarantee &lt;code&gt;read-after-write&lt;/code&gt;, the response for a PUT does not come back until the original object&amp;rsquo;s parity is calculated, split and placed on different nodes (depending on crush map rules). The GET assembles the objects back together and then responds.&lt;/p&gt;

&lt;p&gt;Based on my tests with my multi-petabyte clusters, I have calculated approximately a little less than 20% on 2MB PUTs as compared with the same object using replicas (mileage will vary based on drive types, CPU, network, etc). The GETs were not as bad. So, if you need every ounce of performance you can get then use replicas. If you want to put as much data into the same raw density then use erasure coding. You can also test different size &lt;code&gt;k/m&lt;/code&gt; values to gauge the overall impact on your cluster (you will have to wipe the data if you change the &lt;code&gt;k/m&lt;/code&gt; values).&lt;/p&gt;

&lt;p&gt;I have been asked so often about calculating data/parity combinations that I have put together a free erasure coding k/m value calculator in Excel. I have included it in the &lt;code&gt;chef-bcs&lt;/code&gt; Chef repo at &lt;a href=&#34;https://github.com/bloomberg/chef-bcs/tree/master/cookbooks/chef-bcs/files/default/utilities&#34;&gt;https://github.com/bloomberg/chef-bcs/tree/master/cookbooks/chef-bcs/files/default/utilities&lt;/a&gt;. You can download just the spreadsheet and play with the numbers or fork the repo and automatically build Ceph clusters. Enjoy!&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/bloomberg/chef-bcs/tree/master/cookbooks/chef-bcs/files/default/utilities&#34;&gt;&lt;img src=&#34;http://lambdastack.io/img/erasure-coding.png&#34; style=&#34;max-width: 100%&#34;&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Red Hat Ceph Object Store</title>
      <link>http://lambdastack.io/videos/redhat-objectstore/</link>
      <pubDate>Wed, 15 Feb 2017 18:56:27 -0400</pubDate>
      
      <guid>http://lambdastack.io/videos/redhat-objectstore/</guid>
      <description>

&lt;h4 id=&#34;ceph-object-store&#34;&gt;Ceph Object Store&lt;/h4&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/Wh4N-cka7Uc&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;strong&gt;NB: Disclaimer - This is video from Red Hat - not me&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This video does a good job talking about Object Store. It&amp;rsquo;s also from Red Hat.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ceph and SMR Drives - NO</title>
      <link>http://lambdastack.io/blog/2017/02/13/smr/</link>
      <pubDate>Mon, 13 Feb 2017 11:27:27 -0400</pubDate>
      
      <guid>http://lambdastack.io/blog/2017/02/13/smr/</guid>
      <description>

&lt;h4 id=&#34;don-t-use-ceph-with-smr-drives&#34;&gt;Don&amp;rsquo;t use Ceph with SMR Drives!&lt;/h4&gt;

&lt;p&gt;A while back at Ceph Day in NYC I saw a representative from a drive manufacturer talk about SMR drives. SMR stands for &lt;a href=&#34;https://en.wikipedia.org/wiki/Shingled_magnetic_recording&#34;&gt;&lt;code&gt;Shingled Magnetic Recording&lt;/code&gt;&lt;/a&gt; and the analogy given was &amp;lsquo;think about the shingles on a house&amp;rsquo;. It sounded very interesting but they were not on the market at the time.&lt;/p&gt;

&lt;p&gt;A very active community member, Wido den Hollander, posted his findings &lt;a href=&#34;https://blog.widodh.nl/2017/02/do-not-use-smr-disks-with-ceph/&#34;&gt;here&lt;/a&gt;. The net result was &lt;code&gt;DO NOT USE SMR DRIVES!&lt;/code&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ceph Object Store for Development</title>
      <link>http://lambdastack.io/architectures/objectstore/</link>
      <pubDate>Sun, 01 Jan 2017 13:00:20 +0200</pubDate>
      
      <guid>http://lambdastack.io/architectures/objectstore/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://ceph.com&#34;&gt;Ceph.com&lt;/a&gt; talks about several Reference Architectures. The default Reference Architecture defined in &lt;a href=&#34;https://github.com/bloomberg/chef-bcs&#34;&gt;Chef-BCS&lt;/a&gt; shows how to build out a Ceph Object Store similar to the one we built at Bloomberg. The default Vagrant/VirtualBox version represents the following:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;3 RGW (Radosgw - S3) Nodes&lt;/p&gt;

&lt;p&gt;3 Ceph Mon Nodes&lt;/p&gt;

&lt;p&gt;3 Ceph OSD Nodes&lt;/p&gt;

&lt;p&gt;2 HAProxy Nodes (can use Bootstrap as a failover)&lt;/p&gt;

&lt;p&gt;1 Bootstrap Node&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The bootstrap node is used as a centralize launchpad so to speak where the Chef Server, Cobbler, ISO Linux Images, etc are installed and maintained. The bootstrap node is built first and it drives the building of all of the other nodes in the Ceph cluster. The bootstrap can also be used for other things as well if needed.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://lambdastack.io/img/ObjectStoreRefArch.svg&#34;&gt;&lt;img src=&#34;http://lambdastack.io/img/ObjectStoreRefArch.svg&#34; height=&#34;50%&#34; width=&#34;50%&#34;&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>S3LSIO - S3 Utility</title>
      <link>http://lambdastack.io/blog/2016/10/05/s3lsio_s3/</link>
      <pubDate>Wed, 05 Oct 2016 15:00:00 +0200</pubDate>
      
      <guid>http://lambdastack.io/blog/2016/10/05/s3lsio_s3/</guid>
      <description>&lt;p&gt;The initial release of s3lsio has been released for both MacOSX and Linux (RHEL/Fedora/CentOS). Ubuntu will be released soon. In theory Windows should work but OpenSSL can cause initial setup of the Rust build environment for s3lsio.&lt;/p&gt;

&lt;p&gt;There are constant enhancements being made on a weekly basis. S3lsio is a command line tool that can work within a script, called from an app or ran stand alone to easily manipulate your AWS S3 and Ceph Rados Gateway (S3) environments. In addition to plain output you can specify JSON, pretty-json, serialize or quiet mode (only return codes are output) . Pretty-json is the default.&lt;/p&gt;

&lt;p&gt;Credentials:&lt;/p&gt;

&lt;p&gt;AWS supports a credentials file in the home directory of a given user such as:
~/.aws/credentials&lt;/p&gt;

&lt;p&gt;This file is a standard INI file format. So, if you don&amp;rsquo;t want to enter your credentials via the command line then you can place them in the aws credentials file.&lt;/p&gt;

&lt;p&gt;API Signatures:&lt;/p&gt;

&lt;p&gt;By default AWS uses API Signature V4 however, Ceph (Hammer and below) uses V2 but Ceph (Jewel and higher) supports V4. S3lsio supports both. The default signature is V4 for S3lsio. You can change that by passing in -s V2 or setting &amp;lsquo;signature=V2&amp;rsquo; in the config file.&lt;/p&gt;

&lt;p&gt;Proxies:&lt;/p&gt;

&lt;p&gt;S3lsio supports proxies in 3 ways: 1. Pass in -p &lt;whatever your proxy url:port&gt;, 2. Add to config file, 3. Set http_proxy, https_proxy and no_proxy environment variables. By default proxies are not used.&lt;/p&gt;

&lt;p&gt;Example to create an S3 bucket:&lt;/p&gt;

&lt;p&gt;./s3lsio bucket create mynewbucket&lt;/p&gt;

&lt;p&gt;You can download and more instructions here: &lt;a href=&#34;http://www.lambdastack.io/s3lsio&#34;&gt;http://www.lambdastack.io/s3lsio&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Building Large Ceph Clusters</title>
      <link>http://lambdastack.io/blog/2016/10/01/building_large_ceph_clusters/</link>
      <pubDate>Sat, 01 Oct 2016 13:00:20 +0200</pubDate>
      
      <guid>http://lambdastack.io/blog/2016/10/01/building_large_ceph_clusters/</guid>
      <description>&lt;p&gt;Ceph is a very complex distributed storage system that provides an Object Store, Block Storage Devices and Distributed File System. It has a built-in installation program called Ceph-Deploy but it&amp;rsquo;s design is for very simple and small installations. There are two official automated installation and maintenance systems for Ceph, Ceph-Ansible and Ceph-Chef. As the name implies, Ceph-Ansible is built for the Ansible while Ceph-Chef is built for Chef. I will focus on Ceph-Chef for the Chef environment.&lt;/p&gt;

&lt;p&gt;Ceph-Chef is managed in the Ceph github repo and it&amp;rsquo;s a Chef cookbook that allows for a lot flexibility. Since Ceph-Chef is a cookbook it relies on a larger Chef repo project that you would create and customized for your environment. A great example Chef project that uses Ceph-Chef can be found at Bloomberg&amp;rsquo;s github repo called Chef-BCS. Chef-BCS is what powers the large Ceph installations at Bloomberg.&lt;/p&gt;

&lt;p&gt;The recommend way to get started using both Ceph-Chef and Chef-BCS is to clone Chef-BCS on github.com. Once you have forked it then clone it to your local environment (i.e., an OSX or Linux based environment - have not tested Windows based environment). To just kick the tires so to speak, make sure VirtualBox and Vagrant are installed.&lt;/p&gt;

&lt;p&gt;Once the dependencies are installed; inside the root of the cloned project you can find a &lt;code&gt;build_dev.sh&lt;/code&gt; script (if not present then simply change to the /&lt;your cloned root&gt;/bootstrap/vms/vagrant directory and enter ./CEPH_UP.sh). It will simply launch the default VirtualBox install of Ceph which consists of four VMs (1 - Bootstrap VM and 3 - Ceph VMs). The script also changes the directory to /&lt;your cloned root&gt;/bootstrap/vms/vagrant. Inside that directory you can see Ceph running on each Ceph VM with &amp;lsquo;vagrant ssh ceph-vm1&amp;rsquo; (change vm1 to vm2 or vm3 to see each Ceph VM).&lt;/p&gt;

&lt;p&gt;Now that you have a default Ceph Object Store VM cluster running locally, you can change ceph.conf settings and restart the processes to inspect the changes before building your real cluster. There will be more how-to articles coming soon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Rust AWS S3 SDK</title>
      <link>http://lambdastack.io/blog/2016/09/20/rust_aws_s3_sdk/</link>
      <pubDate>Tue, 20 Sep 2016 15:00:00 +0200</pubDate>
      
      <guid>http://lambdastack.io/blog/2016/09/20/rust_aws_s3_sdk/</guid>
      <description>&lt;p&gt;The aws-sdk-rust library is officially released which allows both V2 and V4 API signatures. This is import for those that wish to use the SDK to access storage products that implement the S3 Interface such as Ceph&amp;rsquo;s Rados Gateway (RGW). Ceph Hammer release only uses V2 while the Jewel and higher releases support V4.&lt;/p&gt;

&lt;p&gt;Enterprise level proxy support has also been added. So, if http_proxy, https_proxy and no_proxy environment variables are in use, aws-sdk-rust will use them to access the S3 resource.&lt;/p&gt;

&lt;p&gt;Aws-sdk-rust can be found on crates.io and &lt;a href=&#34;https://github.com/lambdastackio/aws-sdk-rust&#34;&gt;https://github.com/lambdastackio/aws-sdk-rust&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Keep track of the crate. The library is under heavy development.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ceph Librados for Rust</title>
      <link>http://lambdastack.io/blog/2016/09/05/ceph-rust/</link>
      <pubDate>Mon, 05 Sep 2016 11:27:27 -0400</pubDate>
      
      <guid>http://lambdastack.io/blog/2016/09/05/ceph-rust/</guid>
      <description>

&lt;h4 id=&#34;offical-ceph-rust-librados-library&#34;&gt;Offical Ceph Rust Librados Library&lt;/h4&gt;

&lt;p&gt;The official Ceph librados Rust API has been released called ceph-rust. Ceph-rust can be found at crates.io and at &lt;a href=&#34;https://github.com/ceph/ceph-rust&#34;&gt;https://github.com/ceph/ceph-rust&lt;/a&gt;. Ceph-rust is a very thin layer above the C librados library that drives Ceph. In addition, it has some higher level APIs that wrap the low-level C interface with Rust specific protection. Working with Ceph is now fast and safe.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>OpenStack Summit - Austin TX</title>
      <link>http://lambdastack.io/videos/openstack-austin/</link>
      <pubDate>Fri, 15 Apr 2016 13:00:20 +0200</pubDate>
      
      <guid>http://lambdastack.io/videos/openstack-austin/</guid>
      <description>&lt;p&gt;Talk at OpenStack Summit in Austin TX. I discussed our new Architecture for our Object Store. I also go into (just a little) why a fully hyper-converged Compute/Storage is not a good idea for large installations.&lt;/p&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/gW084yAvoK0&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Ceph Day in NYC</title>
      <link>http://lambdastack.io/videos/cephday-nyc/</link>
      <pubDate>Wed, 20 Jan 2016 13:00:20 +0200</pubDate>
      
      <guid>http://lambdastack.io/videos/cephday-nyc/</guid>
      <description>&lt;p&gt;This talk was with Chris Morgan and myself at Ceph Day in NYC. We talk about what we&amp;rsquo;re doing with Ceph and OpenStack and our new multi-petabyte Object Store called BCS (Bloomberg Cloud Storage).&lt;/p&gt;

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/ZHyIJ1UA5R8?t=13m48s&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Ceph Day - Chicago</title>
      <link>http://lambdastack.io/videos/cephday-chicago/</link>
      <pubDate>Thu, 20 Aug 2015 13:00:27 -0400</pubDate>
      
      <guid>http://lambdastack.io/videos/cephday-chicago/</guid>
      <description>

&lt;h4 id=&#34;ceph-day-chicago&#34;&gt;Ceph Day - Chicago&lt;/h4&gt;

&lt;iframe src=&#34;//www.slideshare.net/slideshow/embed_code/key/qepwEI1tZ53PwO&#34; width=&#34;595&#34; height=&#34;485&#34; frameborder=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;&#34; allowfullscreen&gt; &lt;/iframe&gt;

&lt;p&gt;Ceph Day in Chicago presentation from &lt;a href=&#34;http://www.slideshare.net/Inktank_Ceph/ceph-day-chicago-ceph-at-work-at-bloomberg&#34;&gt;SlideShare&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>