<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ceph on LambdaStack</title>
    <link>http://lambdastack.io/tags/ceph/</link>
    <description>Recent content in Ceph on LambdaStack</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 18 Mar 2017 01:00:00 +0200</lastBuildDate>
    
	<atom:link href="http://lambdastack.io/tags/ceph/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Hardware Sizing - Ceph Radosgw (RGW)</title>
      <link>http://lambdastack.io/blog/2017/03/18/hardware-sizing-ceph-rgw/</link>
      <pubDate>Sat, 18 Mar 2017 01:00:00 +0200</pubDate>
      
      <guid>http://lambdastack.io/blog/2017/03/18/hardware-sizing-ceph-rgw/</guid>
      <description>Hardware Sizing - Ceph Radosgw (RGW) I&amp;rsquo;m often asked, &amp;ldquo;what&amp;rsquo;s the best hardware to use for Ceph?&amp;rdquo; The answer is simple - it depends. With Ceph there are many moving parts such as:
 Ceph Monitor Nodes
  Ceph RGW Nodes
  Ceph Mgr Nodes
  Ceph OSD Nodes
  Ceph MDS Nodes
 In addition to those nodes you may also have software or hardware load-balancers in front of your RGW nodes.</description>
    </item>
    
    <item>
      <title>AWS S3 vs On-Premises</title>
      <link>http://lambdastack.io/blog/2017/03/09/aws-vs-onprem/</link>
      <pubDate>Thu, 09 Mar 2017 20:00:00 +0200</pubDate>
      
      <guid>http://lambdastack.io/blog/2017/03/09/aws-vs-onprem/</guid>
      <description>AWS S3 vs On-Premises When you own Enterprise Storage or are asked to build enterprise class storage you find yourself more frequently having to cost justify against outside Cloud Storage vendors like AWS S3, GCE or Azure. So, how will you do that since you have to make capital expenditures vs OpEx that these providers discuss so much?
When you break down the pricing of AWS S3 and others you find that the actual storage cost is not too bad (depending on amount of stored data).</description>
    </item>
    
    <item>
      <title>Erasure Coding vs Replica</title>
      <link>http://lambdastack.io/blog/2017/02/26/erasure-coding/</link>
      <pubDate>Sun, 26 Feb 2017 11:27:27 -0400</pubDate>
      
      <guid>http://lambdastack.io/blog/2017/02/26/erasure-coding/</guid>
      <description>Erasure Coding vs Replica Ceph RadosGW (RGW), Ceph&amp;rsquo;s S3 Object Store, supports both Replica and Erasure Coding. Most all examples of using RGW show replicas because that&amp;rsquo;s the easiest to setup, manage and get your head around. Replicas simply means that a default of 3 means that RGW stores the original plus two more copies spread out within the cluster based on the Crush Map, Ceph&amp;rsquo;s way of calculating where to store objects.</description>
    </item>
    
    <item>
      <title>Red Hat Ceph Object Store</title>
      <link>http://lambdastack.io/videos/redhat-objectstore/</link>
      <pubDate>Wed, 15 Feb 2017 18:56:27 -0400</pubDate>
      
      <guid>http://lambdastack.io/videos/redhat-objectstore/</guid>
      <description>Ceph Object Store NB: Disclaimer - This is video from Red Hat - not me
This video does a good job talking about Object Store. It&amp;rsquo;s also from Red Hat.</description>
    </item>
    
    <item>
      <title>Ceph and SMR Drives - NO</title>
      <link>http://lambdastack.io/blog/2017/02/13/smr/</link>
      <pubDate>Mon, 13 Feb 2017 11:27:27 -0400</pubDate>
      
      <guid>http://lambdastack.io/blog/2017/02/13/smr/</guid>
      <description>Don&amp;rsquo;t use Ceph with SMR Drives! A while back at Ceph Day in NYC I saw a representative from a drive manufacturer talk about SMR drives. SMR stands for Shingled Magnetic Recordingand the analogy given was &amp;lsquo;think about the shingles on a house&amp;rsquo;. It sounded very interesting but they were not on the market at the time.
A very active community member, Wido den Hollander, posted his findings here. The net result was DO NOT USE SMR DRIVES!</description>
    </item>
    
    <item>
      <title>Ceph Object Store for Development</title>
      <link>http://lambdastack.io/architectures/objectstore/</link>
      <pubDate>Sun, 01 Jan 2017 13:00:20 +0200</pubDate>
      
      <guid>http://lambdastack.io/architectures/objectstore/</guid>
      <description>Ceph.comtalks about several Reference Architectures. The default Reference Architecture defined in Chef-BCSshows how to build out a Ceph Object Store similar to the one we built at Bloomberg. The default Vagrant/VirtualBox version represents the following:
 3 RGW (Radosgw - S3) Nodes
  3 Ceph Mon Nodes
  3 Ceph OSD Nodes
  2 HAProxy Nodes (can use Bootstrap as a failover)
  1 Bootstrap Node</description>
    </item>
    
    <item>
      <title>S3LSIO - S3 Utility</title>
      <link>http://lambdastack.io/blog/2016/10/05/s3lsio_s3/</link>
      <pubDate>Wed, 05 Oct 2016 15:00:00 +0200</pubDate>
      
      <guid>http://lambdastack.io/blog/2016/10/05/s3lsio_s3/</guid>
      <description>The initial release of s3lsio has been released for both MacOSX and Linux (RHEL/Fedora/CentOS). Ubuntu will be released soon. In theory Windows should work but OpenSSL can cause initial setup of the Rust build environment for s3lsio.
There are constant enhancements being made on a weekly basis. S3lsio is a command line tool that can work within a script, called from an app or ran stand alone to easily manipulate your AWS S3 and Ceph Rados Gateway (S3) environments.</description>
    </item>
    
    <item>
      <title>Building Large Ceph Clusters</title>
      <link>http://lambdastack.io/blog/2016/10/01/building_large_ceph_clusters/</link>
      <pubDate>Sat, 01 Oct 2016 13:00:20 +0200</pubDate>
      
      <guid>http://lambdastack.io/blog/2016/10/01/building_large_ceph_clusters/</guid>
      <description>Ceph is a very complex distributed storage system that provides an Object Store, Block Storage Devices and Distributed File System. It has a built-in installation program called Ceph-Deploy but it&amp;rsquo;s design is for very simple and small installations. There are two official automated installation and maintenance systems for Ceph, Ceph-Ansible and Ceph-Chef. As the name implies, Ceph-Ansible is built for the Ansible while Ceph-Chef is built for Chef. I will focus on Ceph-Chef for the Chef environment.</description>
    </item>
    
    <item>
      <title>Rust AWS S3 SDK</title>
      <link>http://lambdastack.io/blog/2016/09/20/rust_aws_s3_sdk/</link>
      <pubDate>Tue, 20 Sep 2016 15:00:00 +0200</pubDate>
      
      <guid>http://lambdastack.io/blog/2016/09/20/rust_aws_s3_sdk/</guid>
      <description>The aws-sdk-rust library is officially released which allows both V2 and V4 API signatures. This is import for those that wish to use the SDK to access storage products that implement the S3 Interface such as Ceph&amp;rsquo;s Rados Gateway (RGW). Ceph Hammer release only uses V2 while the Jewel and higher releases support V4.
Enterprise level proxy support has also been added. So, if http_proxy, https_proxy and no_proxy environment variables are in use, aws-sdk-rust will use them to access the S3 resource.</description>
    </item>
    
    <item>
      <title>Ceph Librados for Rust</title>
      <link>http://lambdastack.io/blog/2016/09/05/ceph-rust/</link>
      <pubDate>Mon, 05 Sep 2016 11:27:27 -0400</pubDate>
      
      <guid>http://lambdastack.io/blog/2016/09/05/ceph-rust/</guid>
      <description>Offical Ceph Rust Librados Library The official Ceph librados Rust API has been released called ceph-rust. Ceph-rust can be found at crates.io and at https://github.com/ceph/ceph-rust. Ceph-rust is a very thin layer above the C librados library that drives Ceph. In addition, it has some higher level APIs that wrap the low-level C interface with Rust specific protection. Working with Ceph is now fast and safe.</description>
    </item>
    
    <item>
      <title>OpenStack Summit - Austin TX</title>
      <link>http://lambdastack.io/videos/openstack-austin/</link>
      <pubDate>Fri, 15 Apr 2016 13:00:20 +0200</pubDate>
      
      <guid>http://lambdastack.io/videos/openstack-austin/</guid>
      <description>Talk at OpenStack Summit in Austin TX. I discussed our new Architecture for our Object Store. I also go into (just a little) why a fully hyper-converged Compute/Storage is not a good idea for large installations.</description>
    </item>
    
    <item>
      <title>Ceph Day in NYC</title>
      <link>http://lambdastack.io/videos/cephday-nyc/</link>
      <pubDate>Wed, 20 Jan 2016 13:00:20 +0200</pubDate>
      
      <guid>http://lambdastack.io/videos/cephday-nyc/</guid>
      <description>This talk was with Chris Morgan and myself at Ceph Day in NYC. We talk about what we&amp;rsquo;re doing with Ceph and OpenStack and our new multi-petabyte Object Store called BCS (Bloomberg Cloud Storage).</description>
    </item>
    
    <item>
      <title>Ceph Day - Chicago</title>
      <link>http://lambdastack.io/videos/cephday-chicago/</link>
      <pubDate>Thu, 20 Aug 2015 13:00:27 -0400</pubDate>
      
      <guid>http://lambdastack.io/videos/cephday-chicago/</guid>
      <description>Ceph Day - Chicago Ceph Day in Chicago presentation from SlideShare.</description>
    </item>
    
  </channel>
</rss>