<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Load Balancers on The LambdaStack Project</title>
    <link>http://lambdastack.io/tags/load-balancers/index.xml</link>
    <description>Recent content in Load Balancers on The LambdaStack Project</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&lt;p&gt;Copyright (c) 2017, Chris Jones; all rights reserved. All comments and views are my own and not my employer&#39;s.&lt;/p&gt;&lt;p&gt;All content originating from Chris Jones is licensed under &lt;a href=&#34;https://creativecommons.org/licenses/by-sa/4.0/&#34;&gt;Attribution-ShareAlike 4.0 International&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;a href=&#34;https://www.linkedin.com/in/cjones303/&#34;&gt;Click here to contact me.&lt;/a&gt;&lt;/p&gt;</copyright>
    <atom:link href="http://lambdastack.io/tags/load-balancers/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>TCP and Load Balancers</title>
      <link>http://lambdastack.io/blog/2017/01/20/tcp_and_load_balancers/</link>
      <pubDate>Fri, 20 Jan 2017 11:54:20 +0200</pubDate>
      
      <guid>http://lambdastack.io/blog/2017/01/20/tcp_and_load_balancers/</guid>
      <description>

&lt;h4 id=&#34;why-do-i-get-a-tcp-rst-return-when-i-try-to-put-or-get-s3-objects&#34;&gt;Why do I get a &lt;code&gt;TCP RST&lt;/code&gt; return when I try to PUT or GET S3 objects?&lt;/h4&gt;

&lt;p&gt;I have a colleague that has made the following statement:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;The data is sacred but, the requests are not!&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This holds very true when dealing with HTTP calls such as S3 GETs and PUTs.&lt;/p&gt;

&lt;p&gt;Recently, we noticed that a client was doing a massive amounts of PUTs and then issuing corresponding GETs on the same S3 objects. In our case, our S3 is from Ceph&amp;rsquo;s RGW S3 and we have a number of load balancers in front of all of the RGW servers. The client had &amp;ldquo;rolled&amp;rdquo; their own S3 Python SDK instead of using AWS&amp;rsquo; official Python SDK. In doing so, they had not factored in best practices of exponential backoffs (retries).&lt;/p&gt;

&lt;p&gt;When running similar tests against the RGW cluster directly (no load balancers), we were never able to replicate which pointed to the network or the load balancers or both. We had tested this on both commercial and open source load balancers.&lt;/p&gt;

&lt;p&gt;TCP RST return values are usually associated with some sort of &amp;ldquo;man-in-the-middle&amp;rdquo; process such as load balancers, proxies or traffic sniffers. In our case, load balancers. By default, we disabled &lt;code&gt;no linger&lt;/code&gt; features which in theory would imply that the load balancers were not holding on to connections from the clients. We also had very low timeouts set as default.&lt;/p&gt;

&lt;p&gt;After spending days testing and letting our tests run for 24 hour periods we found that on the open source version if you specify &lt;code&gt;no linger&lt;/code&gt; explicitly the issue goes away or dramatically reduces to just noise. For the commercial load balancers we upped the timeouts to 40s which addressed the issue.&lt;/p&gt;

&lt;p&gt;In our case, the client did not want to enable retries in their code due to their own reasons. However, for those clients that had used best practices of exponential backoffs, they never saw an issue to begin with. By combining reasonable timeouts and lingering on your load balancers along with educating clients to code in retries, managing your S3 on-premise cluster becomes a lot easier.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>